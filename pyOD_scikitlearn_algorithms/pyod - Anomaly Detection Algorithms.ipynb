{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyOD & scikit-learn Anomaly Detection Algorithms \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. KNN\n",
    "\n",
    "- For an observation, its distance to its kth nearest neighbor could be viewed as the outlying score. It could be viewed as a way to measure the density.\n",
    "\n",
    "- The unsupervised k-NN method computes the distance of an observation, called Euclidean distance, to other observations. Because an isolated data point has a large distance to other observations, it can be seen as an outlier.\n",
    "\n",
    "- Three kNN detectors are supported:\n",
    "    - **largest**: use the distance to the kth neighbor as the outlier score\n",
    "    - **mean**: use the average of all k neighbors as the outlier score\n",
    "    - **median**: use the median of the distance to k neighbors as the outlier score\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "**n_neighbors** (int, optional (default = 5)) – Number of neighbors to use by default for k\n",
    "\n",
    "<img src='images/1.png'>\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn\n",
    "https://towardsdatascience.com/anomaly-detection-with-pyod-b523fc47db9\n",
    "\n",
    "**Paper**: . We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outlier. \n",
    "\n",
    "- A point p in a data set is an outlier with respect to parameters k and d if no more than k points in the data set are at a distance of d or less from p . The distance function can be any metric distance function.\n",
    "\n",
    "- The main benefit of the approach in [KN98] is that it does not require any apriori knowledge of data distributions that the statistical methods \n",
    "\n",
    "- One is a simple nested-loop algorithm with worst-case complexity O(num_dim*N^2) where num_dim is the number of dimensions and N is the number of points in the dataset\n",
    "\n",
    "https://webdocs.cs.ualberta.ca/~zaiane/pub/check/ramaswamy.pdf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Isolation Forest\n",
    "\n",
    "* It uses the scikit-learn library internally. In this method, data partitioning is done using a set of trees. Isolation Forest provides an anomaly score looking at how isolated the point is in the structure. The anomaly score is then used to identify outliers from normal observations\n",
    "\n",
    "* The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "* Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "\n",
    "* This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "* The root is randomly selected, and the idea is that outliers should be lying close to the root\n",
    "\n",
    "* Anomalies are isolated closer to the root of the tree; whereas normal points are isolated at the deeper end of the tree\n",
    "\n",
    "**n_estimators** (int, optional (default=100)) – The number of base estimators in the ensemble.\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "<img src='images/2.png'> <img src='images/3.png'>\n",
    "\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.iforest\n",
    "\n",
    "https://towardsdatascience.com/use-the-isolated-forest-with-pyod-3818eea68f08\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/4.png'> <img src='images/5.png'> <img src='images/6.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper** : isolation means ‘separating an instance from the rest of the instances’\n",
    "\n",
    "* iForest utilizes no distance or density measures to detect anomalies. This eliminates major computational cost of distance calculation in all distance-based methods and density-based methods.\n",
    "\n",
    "* iForest has a linear time complexity with a low constant and a low memory requirement. To our best knowledge, the best-performing existing method achieves only approximate linear time complexity with high memory usage [13].\n",
    "\n",
    "* iForest has the capacity to scale up to handle extremely large data size and high-dimensional problems with a large number of irrelevant attributes.\n",
    "\n",
    "* Since each partition is randomly generated, individual trees are generated with different sets of partitions. We average path lengths over a number of trees to find the expected path length.\n",
    "\n",
    "* Figure 1(c) shows that the average path lengths of xo and xi converge when the number of trees increases. Using 1000 trees, the average path lengths of xo and xi converge to 4.02 and 12.82 respectively. It shows that anomalies are having path lengths shorter than normal instances. \n",
    "\n",
    "<img src='images/7.png'> <img src='images/8.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Outleir Factor\n",
    "\n",
    "* The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. \n",
    "\n",
    "* By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.\n",
    "\n",
    "* Local Outlier Factor (LOF) implemented on scikit-learn library.\n",
    "\n",
    "**n_neighbors** (int, optional (default=20)) – Number of neighbors to use by default for kneighbors queries. If n_neighbors is larger than the number of samples provided, all samples will be used.\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.lof\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paper** should be read detailed\n",
    "\n",
    "* In conclusion, the LOF of a point tells the density of this point compared to the density of its neighbors. If the density of a point is much smaller than the densities of its neighbors (LOF ≫1), the point is far from dense areas and, hence, an outlier.\n",
    "\n",
    "* Adv: A point will be considered as an outlier if it is at a small distance to the extremely dense cluster. The global approach may not consider that point as an outlier. But the LOF can effectively identify the local outliers\n",
    "\n",
    "* Dis: Since LOF is a ratio, it is tough to interpret. There is no specific threshold value above which a point is defined as an outlier. The identification of an outlier is dependent on the problem and the user\n",
    "\n",
    "* Time complexity: O(n2 )\n",
    "\n",
    "https://www.researchgate.net/publication/221214719_LOF_Identifying_Density-Based_Local_Outliers\n",
    "https://towardsdatascience.com/local-outlier-factor-for-anomaly-detection-cc0c770d2ebe#:~:text=Local%20Outlier%20Factor%20(LOF)%20is,point%20is%20an%20outlier%2Fanomaly.&text=The%20LOF%20is%20a%20calculation,of%20other%20points%20later%20on.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/9.png'> <img src='images/10.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/11.png'>\n",
    "<img src='images/12.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/13.png'>\n",
    "<img src='images/14.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/15.png'>\n",
    "<img src='images/16.png'>\n",
    "<img src='images/17.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. One Class SVM\n",
    "\n",
    "* A One-Class Support Vector Machine is an unsupervised learning algorithm that is trained only on the ‘normal’ data. It learns the boundaries of these points and is therefore able to classify any points that lie outside the boundary as, you guessed it, outliers. \n",
    "\n",
    "**my note**: this is semisupervised learning algorithm actually, not absolute an anomaly detection algorithm\n",
    "\n",
    "* One-class SVM detector implemented on scikit-learn library.\n",
    "\n",
    "**kernel** (string, optional (default='rbf')) – Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.\n",
    "\n",
    "**nu** (float, optional) – An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.\n",
    "\n",
    "**gamma** (float, optional (default='auto')) – Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. If gamma is ‘auto’ then 1/n_features will be used instead.\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.ocsvm\n",
    "\n",
    "paper: https://www.researchgate.net/publication/220499623_Estimating_Support_of_a_High-Dimensional_Distribution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CBLOC\n",
    "\n",
    "* The CBLOF operator calculates the outlier score based on cluster-based local outlier factor\n",
    "\n",
    "* CBLOF takes as an input the data set and the cluster model that was generated by a clustering algorithm. It classifies the clusters into small clusters and large clusters using the parameters alpha and beta.  The anomaly score is then calculated based on the size of the cluster the point belongs to as well as the distance to the nearest large cluster.\n",
    "\n",
    "* It classifies the data into small clusters and large clusters. The anomaly score is then calculated based on the size of the cluster the point belongs to, as well as the distance to the nearest large cluster\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof\n",
    "\n",
    "paper: http://www.dis.uniroma1.it/~sassano/STAGE/Outliers.pdf\n",
    "\n",
    "<img src='images/19.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DBSCAN\n",
    "\n",
    "* As is the case in most machine learning algorithms, the model’s behaviour is dictated by several parameters. For dbscan, we’ll touch on three. \n",
    "\n",
    "    * **eps**: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.\n",
    "    * **min_samples**: The minimum number of neighbors a given point should have in order to be classified as a core point. It’s important to note that the point itself is included in the minimum number of samples.\n",
    "    * **metric**: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).\n",
    "    \n",
    "* The algorithm works by computing the distance between every point and all other points. We then place the points into one of three categories.\n",
    "\n",
    "**Core point**: A point with at least min_samples points whose distance with respect to the point is below the threshold defined by epsilon.\n",
    "\n",
    "**Border point**: A point that isn’t in close proximity to at least min_samples points but is close enough to one or more core point. Border points are included in the cluster of the closest core point.\n",
    "\n",
    "**Noise point**: Points that aren’t close enough to core points to be considered border points. Noise points are ignored. That is to say, they aren’t part of any cluster.\n",
    "\n",
    "**Epsilon specified with knn**: We can calculate the distance from each point to its closest neighbour using the NearestNeighbors. The point itself is included in n_neighbors. The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points.\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/20.png'>\n",
    "<img src='images/21.png'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/22.png'>\n",
    "<img src='images/23.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/24.png'>\n",
    "<img src='images/25.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/26.png'>\n",
    "<img src='images/27.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* n overall average runtime complexity of O(nlogn)\n",
    "\n",
    "* worst case run time complexity remains O(n2).\n",
    "\n",
    "* O(n2) memory, whereas a non-matrix based implementation of DBSCAN only needs O(n) memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AutoEncoder\n",
    "\n",
    "* Auto Encoder (AE) is a type of neural networks for learning useful data representations unsupervisedly. Similar to PCA, AE could be used to detect outlying objects in the data by calculating the reconstruction errors. See [BAgg15] Chapter 3 for details.\n",
    "\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.auto_encoder\n",
    "\n",
    "paper: https://www.springer.com/gp/book/9783319475776\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PCA\n",
    "\n",
    "* Principal component analysis (PCA) can be used in detecting outliers. PCA is a linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n",
    "\n",
    "* In this procedure, covariance matrix of the data can be decomposed to orthogonal vectors, called eigenvectors, associated with eigenvalues. The eigenvectors with high eigenvalues capture most of the variance in the data.\n",
    "\n",
    "* Therefore, a low dimensional hyperplane constructed by k eigenvectors can capture most of the variance in the data. However, outliers are different from normal data points, which is more obvious on the hyperplane constructed by the eigenvectors with small eigenvalues.\n",
    "\n",
    "* Therefore, outlier scores can be obtained as the sum of the projected distance of a sample on all eigenvectors. See [BSCSC03,BAgg15] for details.\n",
    "\n",
    "**Score(X)** = Sum of weighted euclidean distance between each sample to the hyperplane constructed by the selected eigenvectors\n",
    "\n",
    "**contamination** (float in (0., 0.5), optional (default=0.1)) – The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.\n",
    "\n",
    "https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.pca\n",
    "\n",
    "https://homepages.laas.fr/owe/METROSEC/DOC/FDM03.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MY NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN -> \t\n",
    "\t-> Bir gözlem için, kinci en yakın komşusuna olan uzaklığı, dıştaki puan olarak görülebilir.\n",
    "\tHer sample'ı K_inci en yakın komşusuna olan mesafesine göre sıralıyoruz ve bu sıralamadaki ilk n puanı aykırı olarak ilan ediyoruz.\n",
    "\t- contamination(0.1)\t:veri setindeki anormal veri sayısı yüzdesi\n",
    "\t- n_neighbors(5)\t\t:yakınlığı ölçmek için dikkate alınacak komşu sayısı\n",
    "\t- method('largest')\t\t:aykırı puan olarak k'inci komşuya olan mesafeyi kullanın\n",
    "\t- metric('minkowski')\t:mesafe hesaplaması için kullanılacak metrik\n",
    "\t- n_jobs(1)\t\t\t\t:komşu araması için çalıştırılacak paralel işlerin sayısı\n",
    "\n",
    "\tFor an observation, its distance to its kth nearest neighbor could be \n",
    "    viewed as the outlying score. It could be viewed as a way to measure\n",
    "    the density.\n",
    "\tThree kNN detectors are supported:\n",
    "    largest: use the distance to the kth neighbor as the outlier score\n",
    "    mean: use the average of all k neighbors as the outlier score\n",
    "    median: use the median of the distance to k neighbors as the outlier score\n",
    "\t\n",
    "\tParameters\n",
    "    ----------\n",
    "\t\n",
    "\tcontamination : float in (0., 0.5), optional (default=0.1)\n",
    "\t\tThe amount of contamination of the data set,\n",
    "\t\ti.e. the proportion of outliers in the data set. Used when fitting to\n",
    "\t\tdefine the threshold on the decision function.\n",
    "\t\t\n",
    "\tn_neighbors - number of neighbors to consider for measuring the proximity\n",
    "\tmethod : str, optional (default='largest')\n",
    "        {'largest', 'mean', 'median'}\n",
    "\n",
    "        - 'largest': use the distance to the kth neighbor as the outlier score\n",
    "        - 'mean': use the average of all k neighbors as the outlier score\n",
    "        - 'median': use the median of the distance to k neighbors as the outlier score\n",
    "\t\t\n",
    "\tmetric : string or callable, default 'minkowski'\n",
    "        metric to use for distance computation. Any metric from scikit-learn\n",
    "        or scipy.spatial.distance can be used.\n",
    "\t\t\n",
    "\t\tValid values for metric are:\n",
    "\n",
    "        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "          'manhattan']\n",
    "\n",
    "\t\t\n",
    "\tn_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "        Affects only kneighbors and kneighbors_graph methods.\n",
    "\n",
    "-------------------------\t\n",
    "Isolation Forest -> \n",
    "\t-> Bir noktanın(sample) yapı içinde ne kadar izole olduğuna bakan bir anormallik puanı sağlar. \n",
    "\tAnormallik puanı daha sonra normal gözlemlerden aykırı değerleri belirlemek için kullanılır.\n",
    "\tBir özelliği rastgele seçerek ve ardından seçilen özelliğin maksimum ve minimum değerleri arasında rastgele bir bölünmüş değer seçerek gözlemleri 'izole eder'.\n",
    "\tYinelemeli bölümleme bir ağaç yapısıyla temsil edilebildiğinden, \n",
    "\tbir örneği izole etmek için gereken bölme sayısı, kök düğümden sonlandırma düğümüne giden yol uzunluğuna eşittir.\n",
    "\tBu tür rastgele ağaçlardan oluşan bir orman üzerinde ortalaması alınan bu yol uzunluğu, normalliğin ve karar işlevimizin bir ölçüsüdür.\n",
    "\tAnormallikler ağacın köküne daha yakın bir yerde izole edilir; normal noktalar ise ağacın daha derin ucunda izole edilmiştir.\n",
    "\n",
    "\tThe IsolationForest 'isolates' observations by randomly selecting a\n",
    "    feature and then randomly selecting a split value between the maximum and\n",
    "    minimum values of the selected feature.\n",
    "    See :cite:`liu2008isolation,liu2012isolation` for details.\n",
    "\n",
    "    Since recursive partitioning can be represented by a tree structure, the\n",
    "    number of splittings required to isolate a sample is equivalent to the path\n",
    "    length from the root node to the terminating node.\n",
    "\n",
    "    This path length, averaged over a forest of such random trees, is a\n",
    "    measure of normality and our decision function.\n",
    "\n",
    "    Random partitioning produces noticeably shorter paths for anomalies.\n",
    "    Hence, when a forest of random trees collectively produce shorter path\n",
    "    lengths for particular samples, they are highly likely to be anomalies.\n",
    "\t\n",
    "\tParameters\n",
    "    ----------\n",
    "    n_estimators : int, optional (default=100)\n",
    "        The number of base estimators in the ensemble.\n",
    "\n",
    "\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e. the proportion\n",
    "        of outliers in the data set. Used when fitting to define the threshold\n",
    "        on the decision function.\n",
    "\n",
    "    bootstrap : bool, optional (default=False)\n",
    "        If True, individual trees are fit on random subsets of the training\n",
    "        data sampled with replacement. If False, sampling without replacement\n",
    "        is performed.\n",
    "\n",
    "    n_jobs : integer, optional (default=1)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "-------------------------\n",
    "Cluster Based LOF (CBLOF):\t\n",
    "\t\n",
    "\t-> Verileri küçük kümeler ve büyük kümeler halinde sınıflandırır. \n",
    "\tAnormallik puanı daha sonra noktanın ait olduğu kümenin büyüklüğüne ve \n",
    "\ten yakın büyük kümeye olan mesafeye göre hesaplanır.\n",
    "\t\n",
    "\t\n",
    "    r\"\"\"The CBLOF operator calculates the outlier score based on cluster-based\n",
    "    local outlier factor.\n",
    "\n",
    "    CBLOF takes as an input the data set and the cluster model that was\n",
    "    generated by a clustering algorithm. It classifies the clusters into small\n",
    "    clusters and large clusters using the parameters alpha and beta.\n",
    "    The anomaly score is then calculated based on the size of the cluster the\n",
    "    point belongs to as well as the distance to the nearest large cluster.\n",
    "\n",
    "    Use weighting for outlier factor based on the sizes of the clusters as\n",
    "    proposed in the original publication. Since this might lead to unexpected\n",
    "    behavior (outliers close to small clusters are not found), it is disabled\n",
    "    by default.Outliers scores are solely computed based on their distance to\n",
    "    the closest large cluster center.\n",
    "\n",
    "    By default, kMeans is used for clustering algorithm instead of\n",
    "    Squeezer algorithm mentioned in the original paper for multiple reasons.\n",
    "\n",
    "    See :cite:`he2003discovering` for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters : int, optional (default=8)\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set,\n",
    "        i.e. the proportion of outliers in the data set. Used when fitting to\n",
    "        define the threshold on the decision function.\n",
    "\n",
    "    alpha : float in (0.5, 1), optional (default=0.9)\n",
    "        Coefficient for deciding small and large clusters. The ratio\n",
    "        of the number of samples in large clusters to the number of samples in\n",
    "        small clusters.\n",
    "\n",
    "    beta : int or float in (1,), optional (default=5).\n",
    "        Coefficient for deciding small and large clusters. For a list\n",
    "        sorted clusters by size `|C1|, \\|C2|, ..., |Cn|, beta = |Ck|/|Ck-1|`\n",
    "\n",
    "\n",
    "    n_jobs : integer, optional (default=1)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "\n",
    "\n",
    "-------------------------\n",
    "Local Outlier Factor \n",
    "\t-> Her örneğin anormallik skoruna Yerel Aykırı Değer Faktörü adı verilir. \n",
    "\tKomşularına göre belirli bir numunenin yerel yoğunluk sapmasını ölçer. \n",
    "\tAnomali skorunun, nesnenin çevresindeki mahalleye göre ne kadar izole olduğuna bağlı olması yereldir. \n",
    "\tDaha kesin olarak, yerellik, uzaklığı yerel yoğunluğu tahmin etmek için kullanılan k-en yakın komşular tarafından verilir.\n",
    "\tBir noktanın LOF'si, komşularının yoğunluğuna kıyasla bu noktanın yoğunluğunu belirtir. \n",
    "\tBir noktanın yoğunluğu komşularının yoğunluklarından (LOF >> 1) çok daha küçükse, nokta yoğun alanlardan uzaktır ve dolayısıyla bir aykırı değerdir.\n",
    "\tBir numunenin yerel yoğunluğu, komşularının yerel yoğunlukları ile karşılaştırılarak, \n",
    "\tkomşularından önemli ölçüde daha düşük yoğunluğa sahip numuneler belirlenebilir. Bunlar aykırı değerler olarak kabul edilir.\n",
    "\t\n",
    "\t\"\"\"Wrapper of scikit-learn LOF Class with more functionalities.\n",
    "    Unsupervised Outlier Detection using Local Outlier Factor (LOF).\n",
    "\n",
    "    The anomaly score of each sample is called Local Outlier Factor.\n",
    "    It measures the local deviation of density of a given sample with\n",
    "    respect to its neighbors.\n",
    "    It is local in that the anomaly score depends on how isolated the object\n",
    "    is with respect to the surrounding neighborhood.\n",
    "    More precisely, locality is given by k-nearest neighbors, whose distance\n",
    "    is used to estimate the local density.\n",
    "    By comparing the local density of a sample to the local densities of\n",
    "    its neighbors, one can identify samples that have a substantially lower\n",
    "    density than their neighbors. These are considered outliers.\n",
    "    See :cite:`breunig2000lof` for details.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_neighbors : int, optional (default=20)\n",
    "        Number of neighbors to use by default for `kneighbors` queries.\n",
    "        If n_neighbors is larger than the number of samples provided,\n",
    "        all samples will be used.\n",
    "\n",
    "    metric : string or callable, default 'minkowski'\n",
    "        metric used for the distance computation. Any metric from scikit-learn\n",
    "        or scipy.spatial.distance can be used.\n",
    "\n",
    "        If 'precomputed', the training input X is expected to be a distance\n",
    "        matrix.\n",
    "\n",
    "        If metric is a callable function, it is called on each\n",
    "        pair of instances (rows) and the resulting value recorded. The callable\n",
    "        should take two arrays as input and return one value indicating the\n",
    "        distance between them. This works for Scipy's metrics, but is less\n",
    "        efficient than passing the metric name as a string.\n",
    "\n",
    "        Valid values for metric are:\n",
    "\n",
    "        - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
    "          'manhattan']\n",
    "\n",
    "        - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
    "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
    "          'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
    "          'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
    "          'sqeuclidean', 'yule']\n",
    "\n",
    "        See the documentation for scipy.spatial.distance for details on these\n",
    "        metrics:\n",
    "        http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e. the proportion\n",
    "        of outliers in the data set. When fitting this is used to define the\n",
    "        threshold on the decision function.\n",
    "\n",
    "    n_jobs : int, optional (default = 1)\n",
    "        The number of parallel jobs to run for neighbors search.\n",
    "        If ``-1``, then the number of jobs is set to the number of CPU cores.\n",
    "        Affects only kneighbors and kneighbors_graph methods.\n",
    "\t\n",
    "\t\n",
    "-------------------------\t\n",
    "one-class SVM \n",
    "\t-> \n",
    "\t\n",
    "\t\"\"\"Wrapper of scikit-learn one-class SVM Class with more functionalities.\n",
    "    Unsupervised Outlier Detection.\n",
    "\n",
    "    Estimate the support of a high-dimensional distribution.\n",
    "\n",
    "    The implementation is based on libsvm.\n",
    "    See http://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection\n",
    "    and :cite:`scholkopf2001estimating`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    kernel : string, optional (default='rbf')\n",
    "         Specifies the kernel type to be used in the algorithm.\n",
    "         It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
    "         a callable.\n",
    "         If none is given, 'rbf' will be used. If a callable is given it is\n",
    "         used to precompute the kernel matrix.\n",
    "\n",
    "    nu : float, optional\n",
    "        An upper bound on the fraction of training\n",
    "        errors and a lower bound of the fraction of support\n",
    "        vectors. Should be in the interval (0, 1]. By default 0.5\n",
    "        will be taken.\n",
    "\n",
    "    gamma : float, optional (default='auto')\n",
    "        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
    "        If gamma is 'auto' then 1/n_features will be used instead.\n",
    "\t\t\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e.\n",
    "        the proportion of outliers in the data set. Used when fitting to\n",
    "        define the threshold on the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (EnrichProjects)",
   "language": "python",
   "name": "pycharm-b64b4bc7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
